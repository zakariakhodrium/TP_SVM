---
title : "TP Apprentissage statistique SVM"
author: "Zakaria KHODRI"
date: "30 Septembre 2024"
format: pdf
jupyter: python3
---
# Objectif du TP

Ce travail pratique (TP) a pour objectif de mettre en œuvre les techniques de classification supervisée à l'aide des **Support Vector Machines (SVM)** sur des données simulées et réelles. Les SVM sont des algorithmes de classification très puissants qui reposent sur la recherche de règles de décision linéaires, représentées par des hyperplans séparateurs dans un espace de grande dimension.

Les objectifs principaux de ce TP sont :

- Appliquer les SVM pour la classification binaire à l'aide de la librairie **scikit-learn**.
- Explorer l'impact des différents noyaux (linéaire, polynomial) et des hyperparamètres comme **C** et $\gamma$ sur la performance du classifieur.
- Expérimenter les SVM dans des contextes de données déséquilibrées.
- Analyser les résultats et la généralisation du modèle.

Nous allons utiliser des jeux de données comme **[Iris](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)** et des visages **[lfw (Labeled Faces in the Wild)](https://vis-www.cs.umass.edu/lfw/)** pour mettre en pratique les méthodes SVM et évaluer leur performance.

# Question 1 : Classification des classes 1 et 2 du dataset Iris avec un noyau linéaire

L'objectif de cette première question est d'utiliser un **SVM** avec un **noyau linéaire** pour classifier les classes 1 (versicolor) et 2 (virginica) du dataset **Iris**, en se basant uniquement sur les deux premières variables du jeu de données : la **longueur des sépales** et la **largeur des sépales**.
```{python}
#| echo: false

import sys
sys.path.append('/home/zack/Apprentissage_Stat/TP_SVM/code')
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from ISLP import confusion_table

from svm_source import *
from sklearn import svm
from sklearn import datasets
from sklearn.utils import shuffle
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.datasets import fetch_lfw_people
from sklearn.decomposition import PCA
from time import time

scaler = StandardScaler()

import warnings

warnings.filterwarnings("ignore")

plt.style.use("ggplot")
```

1. **Chargement et normalisation des données** : Nous commençons par charger le dataset Iris à l'aide de la librairie **scikit-learn**. Ensuite, nous standardisons les données pour s'assurer que les caractéristiques sont sur la même échelle. Cela est particulièrement important pour les SVM, car ils sont sensibles à la variance des données.
```{python}
#| echo : false
iris = datasets.load_iris()  # Get the data and meta data as a dictionary
X = iris.data  # get the data from the dictionary
X = scaler.fit_transform(X)  # standarize the data
y = iris.target  # get the response variables
IRIS = pd.DataFrame(X, columns=list(iris.feature_names))
IRIS["Plant"] = y
IRIS
```

1. **Sélection des variables** : Nous sélectionnons les deux premières variables (longueur et largeur des sépales) et les classes cibles 1 (versicolor) et 2 (virginica), en excluant la classe 0 (setosa).

```{python}
# Select the first two variables : sepal length (cm),sepal width (cm)
X = X[y != 0, :2]
# Select the the targets versicolor (1) and virginica (2)
y = y[y != 0]
```
3. **Visualisation des données** : Une représentation en 2D des données est affichée pour observer la distribution des classes dans l'espace des caractéristiques.
```{python}
#| echo : false
plt.show()
plt.figure(1, figsize=(16, 8))
plot_2d(X, y)
```

4. **Division en ensembles d'entraînement et de test** : Nous divisons les données en un ensemble d'entraînement (50 %) et un ensemble de test (50 %). Cela nous permet d'évaluer la capacité de généralisation du modèle.

```{python}
# split train test
X, y = shuffle(X, y, random_state=18)
# the correspondence between each row in X and its label in y is maintained.
X_iris_train, X_iris_test, y_iris_train, y_iris_test = train_test_split(
    X, y, test_size=0.5, random_state=47
)

```
5. **Entraînement du SVM avec un noyau linéaire** : Le modèle est entraîné sur l'ensemble d'entraînement. Nous ajustons également l'hyperparamètre **C** à l'aide de **GridSearchCV** pour trouver la meilleure valeur de régularisation.
```{python}
clf_linear = SVC(kernel="linear")
clf_linear.fit(X_iris_train, y_iris_train)
print("Score of a linear classifier without hyperparameter tuning is", clf_linear.score(X_iris_test, y_iris_test))

```
```{python}
#| echo: false
parameters = {"kernel": ["linear"], "C": list(np.logspace(-3, 3, 2000))}
clf_linear_grid = GridSearchCV(clf_linear, parameters, n_jobs=-1)
clf_linear_grid.fit(X_iris_train, y_iris_train)
print("Best param using GridSearchCV is", clf_linear_grid.best_estimator_)
```
6. **Évaluation des performances** : Nous évaluons la performance du modèle en mesurant le score de classification (précision) et en affichant la matrice de confusion. Cela permet de vérifier la qualité de la séparation entre les classes.
```{python}
print(
    "Generalization score for linear kernel: %s, %s"
    % (
        clf_linear.score(X_iris_train, y_iris_train),
        clf_linear.score(X_iris_test, y_iris_test),
    )
)
confusion_table(clf_linear.predict(X_iris_test), y_iris_test)
```
Après avoir entraîné le modèle avec un **noyau linéaire**, nous obtenons une précision satisfaisante sur l'ensemble de test. En utilisant **GridSearchCV**, nous avons optimisé le paramètre **C**, qui contrôle le compromis entre la maximisation de la marge et la minimisation des erreurs de classification. Nous avons ensuite observé **une amélioration pas trés significatives** des performances générales du modèle.

La matrice de confusion nous permet de visualiser les erreurs de classification et de vérifier si les deux classes sont correctement distinguées par le modèle.

# Question 2 : Classification avec un noyau polynomial

Dans cette question, nous allons expérimenter un SVM avec un **noyau polynomial** pour la classification des classes 1 et 2 du jeu de données Iris. Contrairement au noyau linéaire, le noyau polynomial permet de séparer les classes dans des espaces de caractéristiques de plus haute dimension, ce qui peut améliorer la classification lorsque les données ne sont pas linéairement séparables.

1. **Entraînement du SVM avec un noyau polynomial** : Nous appliquons un SVM avec un noyau polynomial de différents degrés (1, 2 et 3) pour observer l'impact du degré du polynôme sur les performances du modèle.
```{python}
#| echo : false
Cs = list(np.logspace(-3, 3, 5))
gammas = 10.0 ** np.arange(1, 2)
degrees = np.r_[1, 2, 3]

clf_poly = SVC(kernel="poly")
clf_poly.fit(X_iris_train, y_iris_train)

print(
    "Generalization score both train and test sets for polynomial kernel using : %s, %s"
    % (
        clf_poly.score(X_iris_train, y_iris_train),
        clf_poly.score(X_iris_test, y_iris_test),
    )
)
confusion_table(clf_poly.predict(X_iris_test), y_iris_test)
```
2. **Optimisation des hyperparamètres** : Nous effectuons une recherche de grille (**GridSearchCV**) pour optimiser les hyperparamètres **C**, **gamma**, et **degree** afin d'obtenir les meilleures performances.

```{python}
#| echo : false
parameters = {"kernel": ["poly"], "C": Cs, "gamma": gammas, "degree": degrees}
clf_poly_grid = GridSearchCV(clf_poly, parameters, n_jobs=-1)
clf_poly_grid.fit(X_iris_train, y_iris_train)
clf_poly_grid.best_estimator_
```
3. **Évaluation des performances** : Comme pour le noyau linéaire, nous mesurons la performance du modèle en termes de score de généralisation (précision) et affichons la matrice de confusion.
```{python}
#| echo : false
print(
    "score for train and test sets using GridSearchCV using polynomial kernel: %s, %s"
    % (
        clf_poly_grid.score(X_iris_train, y_iris_train),
        clf_poly_grid.score(X_iris_test, y_iris_test),
    )
)
confusion_table(clf_poly_grid.predict(X_iris_test), y_iris_test)
```
4. **Comparaison des résultats avec le noyau linéaire** : Nous traçons les frontières de décision du modèle linéaire et du modèle polynomial pour visualiser la différence dans la capacité de séparation des classes.

Le score du classifier avec noyau Lineaire donne un score plus élevé que noyau polynomial, donc on le préfere mieux que classifier avec noyau polynomial.

```{python}
#| echo : false
# display your results using frontiere
def f_linear(xx):
    """Classifier: needed to avoid warning due to shape issues"""
    return clf_linear_grid.predict(xx.reshape(1, -1))


def f_poly(xx):
    """Classifier: needed to avoid warning due to shape issues"""
    return clf_poly_grid.predict(xx.reshape(1, -1))


plt.ion()
plt.figure(figsize=(18, 7))
plt.subplot(131)
plot_2d(X, y)
plt.title("iris dataset")

plt.subplot(132)
frontiere(f_linear, X, y)
plt.title("linear kernel")

plt.subplot(133)
frontiere(f_poly, X, y)

plt.title("polynomial kernel")
plt.tight_layout()
plt.draw()
```
# Question 3 : SVM GUI
On utilise le script **svm_gui.py**:

- Ce graphique nous permet d'examiner l'impact du choix du paramètre de régularisation C sur la performance du modèle SVM. Comme illustré dans l'exemple testé (voir figure 6), nous observons qu'une valeur de C plus petite entraîne des marges plus larges. Cela signifie que le modèle est plus tolérant aux erreurs de classification, permettant ainsi à certains points d'être mal classés sans pénaliser excessivement le classifieur. En revanche, lorsque C est élevé, le modèle cherche à minimiser les erreurs, ce qui peut conduire à des marges plus étroites et à un surajustement.

\begin{figure}[H]
    \centering
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/c=1.png}
        \caption*{(a) C = 1}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/c=0.01.png}
        \caption*{(b) C = 0.01}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/c=0.001.png}
        \caption*{(c) C = 0.001}
    \end{minipage}

    \vspace{0.5cm}

    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/c=0.0005.png}
        \caption*{(d) C = 0.0005}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/c=0.0001.png}
        \caption*{(e) C = 0.0001}
    \end{minipage}
\end{figure}

- Dans la dernière figure, tous les points rouges sont mal classés, ce qui soulève des préoccupations quant à l'équilibre des classes dans les données. En effet, ce déséquilibre entre le nombre de points noirs et de points rouges peut influencer la performance du modèle. Les points rouges, ayant un poids négligeable par rapport aux points rouges, sont sous-représentés et, par conséquent, le modèle peut ne pas être en mesure d'apprendre correctement à les classer. Cela souligne l'importance de considérer la distribution des classes lors de l'entraînement des modèles SVM, ainsi que l'utilisation de techniques comme la pondération des classes ou la génération de données synthétiques pour améliorer la représentation des classes minoritaires.

# Question 4 : Classification de visages avec un noyau linéaire

Dans cette question, nous allons utiliser un **SVM avec un noyau linéaire** pour effectuer une classification binaire sur un jeu de données de visages. Le but est de différencier deux personnalités, **Donald Rumsfeld** et **Colin Powell**, à partir des images de la base de données **Labeled Faces in the Wild (LFW)**.

Les étapes de l'expérimentation sont les suivantes :

1. **Chargement et exploration des données** : Nous utilisons la fonction `fetch_lfw_people` de **scikit-learn** pour télécharger et charger le jeu de données LFW, en ne sélectionnant que les images des deux personnalités à classifier.
```{python}
#| echo : false
lfw_people = fetch_lfw_people(
    min_faces_per_person=70,
    resize=0.4,
    color=True,
    funneled=False,
    slice_=None,
    download_if_missing=True,
)
# data_home='.'

# introspect the images arrays to find the shapes (for plotting)
images = lfw_people.images
n_samples, h, w, n_colors = images.shape

# the label to predict is the id of the person
target_names = lfw_people.target_names.tolist()

####################################################################
# Pick a pair to classify such as
# names = ['Tony Blair', 'Colin Powell']
names = ["Donald Rumsfeld", "Colin Powell"]

idx0 = lfw_people.target == target_names.index(names[0])
idx1 = lfw_people.target == target_names.index(names[1])
images = np.r_[images[idx0], images[idx1]]
n_samples = images.shape[0]
y = np.r_[np.zeros(np.sum(idx0)), np.ones(np.sum(idx1))].astype(int)

# plot a sample set of the data
plot_gallery(images, np.arange(12))
plt.show()
```
2. **Extraction des caractéristiques** : Pour simplifier la tâche de classification, nous extrayons une seule caractéristique par image : la **luminosité moyenne**. Cela réduit la dimensionnalité des données tout en conservant une information pertinente pour la tâche.
```{python}
X_img = (np.mean(images, axis=3)).reshape(n_samples, -1)
```
3. **Prétraitement des données** : Les caractéristiques extraites sont standardisées afin que les SVM puissent traiter des données correctement mises à l'échelle.
```{python}
#|echo : false
# Scale features
X_img -= np.mean(X_img, axis=0)
X_img /= np.std(X_img, axis=0)
```
4. **Division en ensembles d'entraînement et de test** : Comme dans les questions précédentes, nous divisons les données en un ensemble d'entraînement et un ensemble de test.
```{python}
# Split data into a half training and half test set
X_img_train, X_img_test, y_train, y_test, images_train, images_test = train_test_split(
    X_img, y, images, test_size=0.5, random_state=0
)
```
```{python}
#| echo : false
indices = np.random.permutation(X_img.shape[0])
train_idx, test_idx = indices[: X_img.shape[0] // 2], indices[X_img.shape[0] // 2 :]
X_img_train, X_img_test = X_img[train_idx, :], X_img[test_idx, :]
y_train, y_test = y[train_idx], y[test_idx]
images_train, images_test = images[train_idx, :, :, :], images[test_idx, :, :, :]
```
5. **Entraînement du SVM avec un noyau linéaire** : Nous entraînons un SVM avec un noyau linéaire et ajustons le paramètre de régularisation **C** pour maximiser la précision de la classification.
```{python}
#| echo : false
print("--- Linear kernel ---")
print("Fitting the classifier to the training set")

t0 = time()

Cs = 10.0 ** np.arange(-5, 6)  # Regularization parameters to try
scores = []

# Loop over all Cs and train the model
for C in Cs:
    clf = SVC(kernel="linear", C=C)  # Create SVM with linear kernel and C
    clf.fit(X_img_train, y_train)  # Fit the model
    scores.append(clf.score(X_img_test, y_test))  # Evaluate and save the score

# Get the best C with the highest score
ind = np.argmax(scores)
best_C = Cs[ind]
print("Best C: {}".format(best_C))

# Plot the scores vs. C values
plt.figure()
plt.plot(Cs, scores)
plt.xlabel("Regularization parameter log(C)")
plt.ylabel("Test set score")
plt.xscale("log")
plt.tight_layout()
plt.show()

print("Best score: {}".format(np.max(scores)))

print("Predicting the people names on the testing set")

t0 = time()
```
6. **Évaluation des performances** : Nous utilisons la matrice de confusion et mesurons la précision pour évaluer la qualité des prédictions sur l'ensemble de test.
```{python}
clf_img = SVC(kernel="linear", C=best_C)
clf_img.fit(X_img_train, y_train)
y_predicted = clf_img.predict(X_img_test)

print("done in %0.3fs" % (time() - t0))
# The chance level is the accuracy that will be reached when constantly predicting the majority class.
print("Chance level : %s" % max(np.mean(y), 1.0 - np.mean(y)))
print("Accuracy : %s" % clf_img.score(X_img_test, y_test))
confusion_table(y_predicted, y_test)
```
7. **Visualisation des résultats** : Nous visualisons les coefficients du classifieur sous forme d'image pour interpréter les caractéristiques apprises par le modèle.
```{python}
#| echo : false
# Qualitative evaluation of the predictions using matplotlib
prediction_titles = [
    title(y_predicted[i], y_test[i], names) for i in range(y_predicted.shape[0])
]
plot_gallery(images_test, prediction_titles, n_row=3, n_col=4)
plt.show()

```
```{python}
#| echo : false
# Look at the coefficients
plt.figure()
plt.imshow(np.reshape(clf_img.coef_, (h, w)))
plt.show()
```
Après avoir entraîné le **SVM avec un noyau linéaire**, nous avons optimisé le paramètre de régularisation **C** pour obtenir une précision maximale. La **précision du modèle** sur l'ensemble de test est satisfaisante et dépasse le niveau de chance. La **matrice de confusion** confirme  que le modèle distingue correctement les deux personnalités dans la majorité des cas.

La visualisation des coefficients du modèle nous donne un aperçu des zones des visages qui influencent le plus la décision du classifieur. Ces coefficients, affichés sous forme d'image, permettent de comprendre quelles parties des visages le modèle utilise pour différencier **Donald Rumsfeld** de **Colin Powell** qui peuvent apparaitre comme la bouche et les sourcils ainsi que les yeux .

# Question 5 : Impact des variables de nuisance sur la classification de visages

Dans cette question, nous étudions l'impact des **variables de nuisance** sur la performance d'un modèle SVM utilisant un noyau linéaire. Les variables de nuisance sont des caractéristiques ajoutées aux données d'origine qui n'ont pas de relation directe avec la tâche de classification, et leur ajout permet de tester la robustesse du modèle.

Les étapes sont les suivantes :

1. **Entraînement du modèle sans variables de nuisance** : Nous commençons par entraîner un SVM sur les données d'origine, sans aucune variable ajoutée. Cela nous permet d'établir une référence de performance.
```{python}
#| echo: false
class SVM_CV:
    def __init__(self):
        self.score = None  # To store the score as an attribute

    def run_svm_cv(self, _X, _y):
        _indices = np.random.permutation(_X.shape[0])
        _train_idx, _test_idx = (
            _indices[: _X.shape[0] // 2],
            _indices[_X.shape[0] // 2 :],
        )
        _X_train, _X_test = _X[_train_idx, :], _X[_test_idx, :]
        _y_train, _y_test = _y[_train_idx], _y[_test_idx]

        _parameters = {"kernel": ["linear"], "C": list(np.logspace(-3, 3, 5))}
        _svr = svm.SVC()
        _clf_linear = GridSearchCV(_svr, _parameters, n_jobs=-1)
        _clf_linear.fit(_X_train, _y_train)

        train_score = _clf_linear.score(_X_train, _y_train)
        test_score = _clf_linear.score(_X_test, _y_test)
        self.score = {"test_score": test_score}  # Store scores in the class attribute

        return print(
            f"Generalization score for linear kernel: {train_score}, {test_score} \n"
        )
```
```{python}
print("Score sans variables de nuisance")
clf_cv = SVM_CV()
clf_cv.run_svm_cv(X_img, y)
```
2. **Ajout de variables de nuisance** : Nous ajoutons des variables aléatoires à chaque image (300 nouvelles caractéristiques), ce qui introduit du bruit dans les données. Nous observons ensuite l'impact de ce bruit sur la performance du modèle.
```{python}
#| echo : false
# On rajoute des variables de nuisances
n_features = X_img.shape[1]
sigma = 1
noise = sigma * np.random.randn(
    n_samples,
    300,
)
X_noisy = np.concatenate((X_img, noise), axis=1)
X_noisy = X_noisy[np.random.permutation(X_img.shape[0])]

X_img_reshaped = X_img.reshape(n_samples, 100, 100)
X_noisy_reshaped = X_noisy.reshape(n_samples, 100, 103)
```
3. **Évaluation des performances avant et après l'ajout des variables de nuisance** : Nous comparons les scores de généralisation du modèle sur les ensembles d'entraînement et de test avant et après l'ajout des variables de nuisance.
```{python}
#| echo : false
print("Score avec variables de nuisance")
clf_cv.run_svm_cv(X_noisy, y)
```
4. **Visualisation des images** : Nous affichons les images d'origine et les images bruitées côte à côte pour visualiser l'effet des variables de nuisance.
```{python}
def plot_images(original_images, noisy_images, n_images=5):
    fig, axes = plt.subplots(2, n_images, figsize=(15, 6))

    for i in range(n_images):
        # Original images
        axes[0, i].imshow(original_images[i], cmap="gray")
        axes[0, i].set_title("Original Image")
        axes[0, i].axis("off")

        # Noisy images
        axes[1, i].imshow(noisy_images[i], cmap="gray")
        axes[1, i].set_title("Noisy Image")
        axes[1, i].axis("off")

    plt.tight_layout()
    plt.show()

plot_images(X_img_reshaped, X_noisy_reshaped)
```
### Analyse des résultats

Après avoir effectué les deux entraînements (avec et sans variables de nuisance), nous observons que l'ajout de variables de nuisance réduit généralement les performances du modèle (**le score passe de 87% à 54%**). Cela est dû au fait que ces variables aléatoires n'apportent aucune information utile pour la tâche de classification, mais ajoutent du bruit, ce qui complique l'apprentissage du modèle.

La visualisation des images originales et bruitées permet de constater l'impact des variables de nuisance sur les données. Les images bruitées contiennent des informations supplémentaires qui ne sont pas pertinentes pour la classification des visages, ce qui entraîne une dégradation des performances du SVM.

# Question 6 : Réduction de dimension avec PCA

Dans cette question, nous allons appliquer l'**analyse en composantes principales (PCA)** pour réduire la dimension des données tout en conservant une part significative de la variance. Cela peut améliorer les performances du modèle SVM en éliminant le bruit et les variables non pertinentes.

Les étapes de cette analyse sont les suivantes :

1. **Calcul de la variance expliquée cumulée** : Nous effectuons une PCA sur les données bruitées pour déterminer combien de composantes principales sont nécessaires pour expliquer 95 % de la variance. Cela nous aide à choisir le nombre optimal de composantes à conserver.
**Visualisation de la variance expliquée** : Nous traçons un graphique montrant la variance cumulée expliquée par chaque composante principale.
```{python}
pca = PCA(svd_solver="randomized", random_state=12).fit(X_noisy)
var_cumu = np.cumsum(pca.explained_variance_ratio_) * 100


def num_components_exp_var(exp_var):
    var_cumu = np.cumsum(pca.explained_variance_ratio_) * 100
    # How many PCs explain exp_var % of the variance?
    k = np.argmax(var_cumu > exp_var)
    return k


def dim_reduction(exp_var):
    plt.figure(figsize=[10, 5])
    plt.title(
        f"Cumulative Explained Variance by {num_components_exp_var(exp_var)} components is {exp_var}%"
    )
    plt.ylabel("Cumulative Explained variance (%)")
    plt.xlabel("Principal components")
    plt.axvline(x=num_components_exp_var(exp_var), color="k", linestyle="--")
    plt.axhline(y=exp_var, color="r", linestyle="--")
    ax = plt.plot(var_cumu)
    return plt.show()
dim_reduction(95)
```
2. **Réduction de dimension** : En utilisant le nombre optimal de composantes trouvées, nous transformons les données bruitées en un espace de dimension réduite de **192**.
```{python}
n_components = num_components_exp_var(95)
X_noisy_stand = scaler.fit_transform(X_noisy)
pca2 = PCA(n_components=n_components, svd_solver="randomized").fit(X_noisy_stand)
X_pca = pca2.transform(X_noisy_stand)
```
3. **Évaluation de la performance du SVM après réduction de dimension** : Nous entraînons un SVM avec les données réduites et mesurons sa performance.
```{python}
print("Score after dimensionality reduction")
clf_cv.run_svm_cv(X_pca, y)
```
4. **Visualisation des images images compressées**: Nous affichons les images avant et après la réduction de dimension pour illustrer la transformation.
```{python}
plot_images(X_img_reshaped, X_pca.reshape(357, 12, 16), 5)
```
5. **Variation du score avec la variation des nombres des composantes**
La fonction plot_pca_vs_score évalue l'impact du nombre de composantes principales sur les performances d'un modèle SVM. Elle standardise d'abord les données, puis applique la PCA avec différents nombres de composantes, en entraînant et en évaluant le modèle pour chaque configuration. Enfin, elle trace un graphique illustrant la relation entre le nombre de composantes et le score SVM, avec des barres d'erreur représentant l'écart type des scores.

```{python}
def plot_pca_vs_score(X, y, start_comp, end_comp, step, num_runs):
    """
    Function to plot the SVM scores as a function of the number of PCA components.

    Parameters:
    - X: Input features (before PCA)
    - y: Labels
    - start_comp: The starting number of components to test
    - end_comp: The ending number of components to test
    - step: Step size for the range of PCA components
    - num_runs: Number of times to run SVM for each component to observe score variations
    """
    scores_avg = []
    scores_std = []
    num_components_list = list(
        range(start_comp, end_comp, step)
    )  # Range of components to test

    # Standardize the data
    X_stand = scaler.fit_transform(X)

    for n_components in num_components_list:
        run_scores = []

        for run in range(num_runs):
            # Apply PCA with n_components
            pca = PCA(
                n_components=n_components, svd_solver="randomized", random_state=17
            )
            X_pca = pca.fit_transform(X_stand)

            print(f"Running SVM with {n_components} components (Run {run + 1})...")
            # Run SVM on the transformed data and capture the score
            clf_cv.run_svm_cv(X_pca, y)
            score = clf_cv.score["test_score"]
            run_scores.append(score)

        # Compute the average and standard deviation of scores across the runs
        avg_score = sum(run_scores) / len(run_scores)
        std_score = np.std(run_scores)

        scores_avg.append(avg_score)
        scores_std.append(std_score)

    # Plot the number of components vs. the SVM score (with error bars for variations)
    plt.figure(figsize=(10, 6))
    plt.errorbar(
        num_components_list, scores_avg, yerr=scores_std, fmt="-o", color="b", capsize=5
    )
    plt.xlabel("Number of PCA Components")
    plt.ylabel("SVM Score")
    plt.title(f"SVM Score vs. Number of PCA Components (Averaged over {num_runs} runs)")
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    return scores_avg, scores_std
```
```{python}
# plot_pca_vs_score(X_noisy, y=y, start_comp=80, end_comp=190, step=1, num_runs=100)
```
Ici on teste le modèle SVM avec un nombre de composantes allant de 80 à 190, en réalisant 100 exécutions pour chaque configuration afin de capturer la variabilité des scores. Enfin, elle trace un graphique illustrant la relation entre le nombre de composantes et le score SVM, facilitant ainsi l'optimisation du modèle pour améliorer ses performances.

![Score du modèle en changeant les nombre des composantes](Images/svm_score_pca.png)

On peut constater que le modèle aprés la réduction de dimension a un score pas mal qui est plus ou moins mieux que le score avant la réduction de dimension en moyenne de **57%**, donc le modèle n'arrive pas a trouver un signal dans les images bruitées à cause de la destructions des correspondance lorsqu'on a ajouter du bruit et la permutation des pixels dans les images.







